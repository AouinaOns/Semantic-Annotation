{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bffa2e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer, KeyphraseTfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f62b60cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"fr_dep_news_trf\")\n",
    "\n",
    "\n",
    "def print_token_attributes(doc):\n",
    "    for token in doc:\n",
    "        print(\n",
    "            f\"Token: {token.text}\\n\"\n",
    "            f\"Lemma: {token.lemma_}\\n\"\n",
    "            f\"POS: {token.pos_}\\n\"\n",
    "            f\"Tag: {token.tag_}\\n\"\n",
    "            f\"Dependency: {token.dep_}\\n\"\n",
    "            f\"Shape: {token.shape_}\\n\"\n",
    "            f\"Is Alpha: {token.is_alpha}\\n\"\n",
    "            f\"Is Stop: {token.is_stop}\\n\"\n",
    "            \"-----------------------\"\n",
    "        )\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    doc = nlp(text)\n",
    "    filtered_text = \" \".join(token.text for token in doc if not token.is_stop)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "\n",
    "vectorizer1 = KeyphraseCountVectorizer(spacy_pipeline=nlp,pos_pattern='<ADJ.*>*<DET.*>*<NOUN.*>+<ADJ.?>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c2238",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, 'r',encoding=\"utf-8\") as f:\n",
    "        file=f.read()\n",
    "    return file\n",
    "\n",
    "\n",
    "def get_core_section(result):\n",
    "    regex = r\"^[A-Z][A-Z-:’'\\n\\s]+$\\n\"\n",
    "    # retourne le contenu de la section\n",
    "    matches = re.finditer(regex, result, re.MULTILINE)\n",
    "\n",
    "    crh_tlt = [\"Compte rendu d’hospitalisation\"]\n",
    "    crh_parts = []\n",
    "    start_core = 0\n",
    "    end_core = 0\n",
    "    prev_start = 0\n",
    "    for matchNum, match in enumerate(matches, start=1):\n",
    "        end_core = match.start() - 1\n",
    "        start = match.start()\n",
    "        end = match.end()\n",
    "        # print (\"Match {matchNum} was found at {start}-{end}: {match}\".format(matchNum = matchNum, start = match.start(), end = match.end(), match = match.group()))\n",
    "        crh_tlt.append(match.group())\n",
    "        # matches\n",
    "        if matchNum == 1:\n",
    "            text_core = result[start_core:start - 1]\n",
    "            crh_parts.append(text_core)\n",
    "            prev_start = end + 1\n",
    "        else:\n",
    "\n",
    "            text_core = result[prev_start:start - 1]\n",
    "            start = start - 1\n",
    "            crh_parts.append(text_core)\n",
    "\n",
    "\n",
    "        prev_start = end\n",
    "\n",
    "    text_core = result[prev_start:]\n",
    "    crh_parts.append(text_core)\n",
    "    lst_tuple = list(zip(crh_tlt, crh_parts))\n",
    "    #return tuple list of pds section and sections name\n",
    "    return lst_tuple\n",
    "\n",
    "\n",
    "\n",
    "path_output = \"/Users/oaouina/Applications/brat_Annotator/data/processed_crh\"\n",
    "# now we will extract from each document its important np\n",
    "count=0\n",
    "doc_section=[]\n",
    "for filename in os.listdir(path_output):\n",
    "            list_core_sections=[]\n",
    "            pds_text = \"\"\n",
    "            #for each pds we will extract np from each section\n",
    "            # Check whether file is in text format or not\n",
    "            keyphrases_l = []\n",
    "            if filename.endswith(\".txt\"):\n",
    "\n",
    "\n",
    "                #if count==1:\n",
    "                    #break\n",
    "                count += 1\n",
    "\n",
    "                file_path = os.path.join(path_output, filename)\n",
    "                # get the full pds\n",
    "                fulltext=read_text_file(file_path)\n",
    "                # extract the sections\n",
    "\n",
    "                list_core_sections=get_core_section(fulltext)\n",
    "                log_writing(\"############section beg ####################\")\n",
    "                for name_sct ,core_sct in list_core_sections:\n",
    "                    print(\"beginning to extract NPs\")\n",
    "                    #extract sentences\n",
    "                    sentences = extract_sentences(core_sct)\n",
    "                    for i, sentence in enumerate(sentences, 1):\n",
    "                        #now we will extract the keyphrases from sentences wich can cost in performances but it's\n",
    "                        keyphrases = extract_SIFRank_plus(sentence)\n",
    "                        # keyphrases_l contain keyphrases of a text\n",
    "                        keyphrases_l.extend(keyphrases)\n",
    "\n",
    "                keyphrases_only=keyphrases_l\n",
    "                if len(keyphrases_l)>0:\n",
    "                    unique_kp = []\n",
    "                    [unique_kp.append(item) for item in keyphrases_l if item not in unique_kp]\n",
    "                    keyphrases_only= [item for item,score in unique_kp if len(item) > 2 and score > 0.5 and '<rue />' not in item and '<nom />' not in item and \"/>\" not in item and \"<\" not in item ]\n",
    "                list_matches=[]\n",
    "                for kp in keyphrases_only:\n",
    "                        list_matches.extend(find_chunks_with_regex(fulltext,kp))\n",
    "                filtered_entities=keep_longest_overlapping_entities(list_matches)\n",
    "                write_brat_annotation(file_path,filtered_entities)\n",
    "\n",
    "\n",
    "            #print(list_matches)\n",
    "            print(filename)\n",
    "\n",
    "            #unique_strings = remove_included_strings(list_matches)\n",
    "            print(\"Finaal results ############################\")\n",
    "                #print(unique_strings)\n",
    "                \n",
    "def extract_sentences(text):\n",
    "    # Replace line breaks with spaces to keep the sentences together\n",
    "    text = text.replace('\\n', ' ')\n",
    "    # Split the text into sentences using periods as delimiters\n",
    "    sentences = re.split(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<=\\.)(\\s|$)', text)\n",
    "\n",
    "    # Remove empty strings and extra whitespace from each sentence\n",
    "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
    "\n",
    "    return sentences\n",
    "\n",
    "\n",
    "def extract_SIFRank_plus(text_fr):\n",
    "\n",
    "    text = \"Discrete output feedback sliding mode control of second order systems - a moving switching line approach The sliding mode control systems (SMCS) for which the switching variable is designed independent of the initial conditions are known to be sensitive to parameter variations and extraneous disturbances during the reaching phase. For second order systems this drawback is eliminated by using the moving switching line technique where the switching line is initially designed to pass the initial conditions and is subsequently moved towards a predetermined switching line. In this paper, we make use of the above idea of moving switching line together with the reaching law approach to design a discrete output feedback sliding mode control. The main contributions of this work are such that we do not require to use system states as it makes use of only the output samples for designing the controller. and by using the moving switching line a low sensitivity system is obtained through shortening the reaching phase. Simulation results show that the fast output sampling feedback guarantees sliding motion similar to that obtained using state feedback\"\n",
    "\n",
    "\n",
    "    #keyphrases = SIFRank(text_fr, SIF, en_model, N=25,elmo_layers_weight=elmo_layers_weight)\n",
    "    keyphrases_ = SIFRank_plus(text_fr, SIF, en_model, N=15, elmo_layers_weight=elmo_layers_weight)\n",
    "\n",
    "    #log_writing(f' Candidate keyphrase  SIFRank : {keyphrases}')\n",
    "    log_writing(f' Candidate keyphrase  SIFRank_plus : {keyphrases_}')\n",
    "    return keyphrases_\n",
    "\n",
    "\n",
    "def keep_longest_overlapping_entities(entities):\n",
    "    # Sort entities by their start position in ascending order\n",
    "    entities.sort(key=lambda x: x[1])\n",
    "\n",
    "    # List to store the filtered entities (longest non-overlapping entities)\n",
    "    filtered_entities = []\n",
    "\n",
    "    for entity in entities:\n",
    "        # Check if the current entity overlaps with any entity in the filtered list\n",
    "        overlap = False\n",
    "        for filtered_entity in filtered_entities:\n",
    "            if (entity[1] <= filtered_entity[2]) and (entity[2] >= filtered_entity[1]):\n",
    "                overlap = True\n",
    "                # Compare the lengths of the entities and keep the longest one\n",
    "                if entity[2] - entity[1] > filtered_entity[2] - filtered_entity[1]:\n",
    "                    filtered_entities.remove(filtered_entity)\n",
    "                    filtered_entities.append(entity)\n",
    "                break\n",
    "\n",
    "        # If the current entity doesn't overlap with any entity in the filtered list, add it\n",
    "        if not overlap:\n",
    "            filtered_entities.append(entity)\n",
    "\n",
    "    return filtered_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a978d20",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m vectorizer2 \u001b[38;5;241m=\u001b[39m KeyphraseTfidfVectorizer(spacy_pipeline\u001b[38;5;241m=\u001b[39mnlp,pos_pattern\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<ADJ.*>*<DET.*>*<NOUN.*>+<ADJ.*>+\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# the following calls use the nlp object\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m vectorizer1\u001b[38;5;241m.\u001b[39mfit(\u001b[43mdocs\u001b[49m)\n\u001b[1;32m      5\u001b[0m vectorizer2\u001b[38;5;241m.\u001b[39mfit(docs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "vectorizer2 = KeyphraseTfidfVectorizer(spacy_pipeline=nlp,pos_pattern='<ADJ.*>*<DET.*>*<NOUN.*>+<ADJ.*>+')\n",
    "\n",
    "# the following calls use the nlp object\n",
    "vectorizer1.fit(docs)\n",
    "vectorizer2.fit(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e64a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_output = \"/Users/oaouina/Applications/brat_Annotator/data/processed_crh\"\n",
    "# now we will extract from each document its important np\n",
    "count=0\n",
    "doc_section=[]\n",
    "for filename in os.listdir(path_output):\n",
    "            list_core_sections=[]\n",
    "            pds_text = \"\"\n",
    "            #for each pds we will extract np from each section\n",
    "            # Check whether file is in text format or not\n",
    "            keyphrases_l = []\n",
    "            if filename.endswith(\".txt\"):\n",
    "\n",
    "\n",
    "                #if count==1:\n",
    "                    #break\n",
    "                count += 1\n",
    "\n",
    "                file_path = os.path.join(path_output, filename)\n",
    "                # get the full pds\n",
    "                fulltext=read_text_file(file_path)\n",
    "                # extract the sections\n",
    "\n",
    "                list_core_sections=get_core_section(fulltext)\n",
    "                log_writing(\"############section beg ####################\")\n",
    "                for name_sct ,core_sct in list_core_sections:\n",
    "                    print(\"beginning to extract NPs\")\n",
    "                    #extract sentences\n",
    "                    sentences = extract_sentences(core_sct)\n",
    "                    for i, sentence in enumerate(sentences, 1):\n",
    "                        #now we will extract the keyphrases from sentences wich can cost in performances but it's\n",
    "                        keyphrases = extract_SIFRank_plus(sentence)\n",
    "                        # keyphrases_l contain keyphrases of a text\n",
    "                        keyphrases_l.extend(keyphrases)\n",
    "\n",
    "                keyphrases_only=keyphrases_l\n",
    "                if len(keyphrases_l)>0:\n",
    "                    unique_kp = []\n",
    "                    [unique_kp.append(item) for item in keyphrases_l if item not in unique_kp]\n",
    "                    keyphrases_only= [item for item,score in unique_kp if len(item) > 2 and score > 0.5 and '<rue />' not in item and '<nom />' not in item and \"/>\" not in item and \"<\" not in item ]\n",
    "                list_matches=[]\n",
    "                for kp in keyphrases_only:\n",
    "                        list_matches.extend(find_chunks_with_regex(fulltext,kp))\n",
    "                filtered_entities=keep_longest_overlapping_entities(list_matches)\n",
    "                write_brat_annotation(file_path,filtered_entities)\n",
    "\n",
    "\n",
    "            #print(list_matches)\n",
    "            print(filename)\n",
    "\n",
    "            #unique_strings = remove_included_strings(list_matches)\n",
    "            print(\"Finaal results ############################\")\n",
    "                #print(unique_strings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
